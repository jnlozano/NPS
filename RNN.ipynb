{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed883a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attribution: this is a highly modified (improved?) version of the Pytorch tutorial here: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import time\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import matplotlib.ticker as ticker\n",
    "import unicodedata\n",
    "import string\n",
    "import pandas as pd\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'!@#$%^&*()_+1234567890-=/\\<>{}\" #added additional characters\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    #line = str(line) #added str since some passwords are just numbers or nan\n",
    "    tensor = torch.zeros(len(line), 1, n_letters) \n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51c68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if passwords dont get passed through lineToTensor\n",
    "df = pd.read_csv('data.csv', on_bad_lines = 'skip',na_filter=False)\n",
    "for i in range(len(df.password)):\n",
    "    lineToTensor(df.password[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b11cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PasswordData(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_csv(path, on_bad_lines ='skip',na_filter=False) #Theres a nan that gets read in\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx and idx < self.df.shape[0]\n",
    "        pswrd = self.df['password'][idx]\n",
    "        strength = self.df['strength'][idx]\n",
    "        pswrd_tensor = lineToTensor(pswrd)\n",
    "        strength_tensor = torch.tensor([strength], dtype=torch.long)\n",
    "        return pswrd_tensor, strength_tensor, pswrd, strength\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79038ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "passworddata = PasswordData(\"data.csv\")\n",
    "train_size =.8\n",
    "test_size = .2\n",
    "torch.manual_seed(0)\n",
    "train_size = round(.8*len(passworddata))\n",
    "training_data, testing_data = torch.utils.data.random_split(passworddata, [train_size, len(passworddata)-train_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48925ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #note that the batch size must be 1 here because the tensors \n",
    "#cannot be collated (i.e. concatenated along a single axis)\n",
    "# Create data loaders.\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (example, label, length)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and label/length are scalars\n",
    "    \"\"\"\n",
    "    return ([datum[i] for datum in data] for i in range(len(data[0])))\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445a013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# use cpu or gpu device if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size, device=device)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, 3).to(device) #0,1,2 for password strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4eba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader)\n",
    "    for ind, (X_list, y_list, _, _) in enumerate(dataloader): #iterate over the dataloader\n",
    "        loss = 0.0\n",
    "        batch_size = len(X_list)\n",
    "        for batch_ind in range(batch_size): #iterate over the batch\n",
    "            X, y = X_list[batch_ind].to(device), y_list[batch_ind].to(device)\n",
    "            hidden = model.initHidden()\n",
    "            for i in range(X.size()[0]): #iterate over the letters in the name\n",
    "                output, hidden = model(X[i], hidden)\n",
    "                \n",
    "            loss += loss_fn(output, y)\n",
    "    \n",
    "        loss /= batch_size\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ind % 100 == 0:\n",
    "            print(f\"loss: {loss.item():>7f}  [{ind:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162e4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_list, y_list, _, _ in dataloader:\n",
    "            batch_size = len(X_list)\n",
    "            loss = 0.0\n",
    "            for batch_ind in range(batch_size):\n",
    "                X, y = X_list[batch_ind].to(device), y_list[batch_ind].to(device)\n",
    "                hidden = model.initHidden()\n",
    "\n",
    "                for i in range(X.size()[0]):\n",
    "                    output, hidden = model(X[i], hidden)\n",
    "    \n",
    "                test_loss += loss_fn(output, y).item()\n",
    "                correct += (output.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a14027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.092560  [    0/ 8371]\n",
      "loss: 0.969427  [  100/ 8371]\n",
      "loss: 0.886634  [  200/ 8371]\n",
      "loss: 0.787420  [  300/ 8371]\n",
      "loss: 0.874267  [  400/ 8371]\n",
      "loss: 0.730038  [  500/ 8371]\n",
      "loss: 0.684770  [  600/ 8371]\n",
      "loss: 0.782009  [  700/ 8371]\n",
      "loss: 0.832208  [  800/ 8371]\n",
      "loss: 0.676518  [  900/ 8371]\n",
      "loss: 0.810301  [ 1000/ 8371]\n",
      "loss: 0.752651  [ 1100/ 8371]\n",
      "loss: 0.799870  [ 1200/ 8371]\n",
      "loss: 0.647170  [ 1300/ 8371]\n",
      "loss: 0.781608  [ 1400/ 8371]\n",
      "loss: 0.755116  [ 1500/ 8371]\n",
      "loss: 0.730730  [ 1600/ 8371]\n",
      "loss: 0.887379  [ 1700/ 8371]\n",
      "loss: 0.818258  [ 1800/ 8371]\n",
      "loss: 0.864792  [ 1900/ 8371]\n",
      "loss: 0.635539  [ 2000/ 8371]\n",
      "loss: 0.640721  [ 2100/ 8371]\n",
      "loss: 0.806593  [ 2200/ 8371]\n",
      "loss: 0.697405  [ 2300/ 8371]\n",
      "loss: 0.770345  [ 2400/ 8371]\n",
      "loss: 0.703306  [ 2500/ 8371]\n",
      "loss: 0.742807  [ 2600/ 8371]\n",
      "loss: 0.716578  [ 2700/ 8371]\n",
      "loss: 0.610231  [ 2800/ 8371]\n",
      "loss: 0.586180  [ 2900/ 8371]\n",
      "loss: 0.572462  [ 3000/ 8371]\n",
      "loss: 0.716620  [ 3100/ 8371]\n",
      "loss: 0.785748  [ 3200/ 8371]\n",
      "loss: 0.908473  [ 3300/ 8371]\n",
      "loss: 0.716538  [ 3400/ 8371]\n",
      "loss: 0.600757  [ 3500/ 8371]\n",
      "loss: 0.713115  [ 3600/ 8371]\n",
      "loss: 0.709914  [ 3700/ 8371]\n",
      "loss: 0.642664  [ 3800/ 8371]\n",
      "loss: 0.689197  [ 3900/ 8371]\n",
      "loss: 0.616345  [ 4000/ 8371]\n",
      "loss: 0.680037  [ 4100/ 8371]\n",
      "loss: 0.781919  [ 4200/ 8371]\n",
      "loss: 0.708453  [ 4300/ 8371]\n",
      "loss: 0.605767  [ 4400/ 8371]\n",
      "loss: 0.574518  [ 4500/ 8371]\n",
      "loss: 0.586392  [ 4600/ 8371]\n",
      "loss: 0.731354  [ 4700/ 8371]\n",
      "loss: 0.697763  [ 4800/ 8371]\n",
      "loss: 0.624216  [ 4900/ 8371]\n",
      "loss: 0.663034  [ 5000/ 8371]\n",
      "loss: 0.722751  [ 5100/ 8371]\n",
      "loss: 0.608941  [ 5200/ 8371]\n",
      "loss: 0.738248  [ 5300/ 8371]\n",
      "loss: 0.839529  [ 5400/ 8371]\n",
      "loss: 0.659874  [ 5500/ 8371]\n",
      "loss: 0.585489  [ 5600/ 8371]\n",
      "loss: 0.569279  [ 5700/ 8371]\n",
      "loss: 0.662178  [ 5800/ 8371]\n",
      "loss: 0.701546  [ 5900/ 8371]\n",
      "loss: 0.765684  [ 6000/ 8371]\n",
      "loss: 0.572035  [ 6100/ 8371]\n",
      "loss: 0.563944  [ 6200/ 8371]\n",
      "loss: 0.822788  [ 6300/ 8371]\n",
      "loss: 0.687884  [ 6400/ 8371]\n",
      "loss: 0.582763  [ 6500/ 8371]\n",
      "loss: 0.678643  [ 6600/ 8371]\n",
      "loss: 0.650570  [ 6700/ 8371]\n",
      "loss: 0.616738  [ 6800/ 8371]\n",
      "loss: 0.465521  [ 6900/ 8371]\n",
      "loss: 0.823249  [ 7000/ 8371]\n",
      "loss: 0.634246  [ 7100/ 8371]\n",
      "loss: 0.799542  [ 7200/ 8371]\n",
      "loss: 0.583165  [ 7300/ 8371]\n",
      "loss: 0.639927  [ 7400/ 8371]\n",
      "loss: 0.569068  [ 7500/ 8371]\n",
      "loss: 0.549443  [ 7600/ 8371]\n",
      "loss: 0.808349  [ 7700/ 8371]\n",
      "loss: 0.625019  [ 7800/ 8371]\n",
      "loss: 0.638644  [ 7900/ 8371]\n",
      "loss: 0.417929  [ 8000/ 8371]\n",
      "loss: 0.744738  [ 8100/ 8371]\n",
      "loss: 0.662101  [ 8200/ 8371]\n",
      "loss: 0.541232  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.555305 \n",
      "\n",
      "Epoch ran in 1436.50 seconds\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.420340  [    0/ 8371]\n",
      "loss: 0.469692  [  100/ 8371]\n",
      "loss: 0.394686  [  200/ 8371]\n",
      "loss: 0.657043  [  300/ 8371]\n",
      "loss: 0.358922  [  400/ 8371]\n",
      "loss: 0.515908  [  500/ 8371]\n",
      "loss: 0.257504  [  600/ 8371]\n",
      "loss: 0.282776  [  700/ 8371]\n",
      "loss: 0.428173  [  800/ 8371]\n",
      "loss: 0.211505  [  900/ 8371]\n",
      "loss: 0.142783  [ 1000/ 8371]\n",
      "loss: 0.204372  [ 1100/ 8371]\n",
      "loss: 0.089388  [ 1200/ 8371]\n",
      "loss: 0.062988  [ 1300/ 8371]\n",
      "loss: 0.045155  [ 1400/ 8371]\n",
      "loss: 0.025383  [ 1500/ 8371]\n",
      "loss: 0.031690  [ 1600/ 8371]\n",
      "loss: 0.016288  [ 1700/ 8371]\n",
      "loss: 0.015834  [ 1800/ 8371]\n",
      "loss: 0.013879  [ 1900/ 8371]\n",
      "loss: 0.006720  [ 2000/ 8371]\n",
      "loss: 0.006092  [ 2100/ 8371]\n",
      "loss: 0.030203  [ 2200/ 8371]\n",
      "loss: 0.007709  [ 2300/ 8371]\n",
      "loss: 0.002072  [ 2400/ 8371]\n",
      "loss: 0.018606  [ 2500/ 8371]\n",
      "loss: 0.001957  [ 2600/ 8371]\n",
      "loss: 0.005454  [ 2700/ 8371]\n",
      "loss: 0.002030  [ 2800/ 8371]\n",
      "loss: 0.003621  [ 2900/ 8371]\n",
      "loss: 0.001478  [ 3000/ 8371]\n",
      "loss: 0.000588  [ 3100/ 8371]\n",
      "loss: 0.003284  [ 3200/ 8371]\n",
      "loss: 0.001503  [ 3300/ 8371]\n",
      "loss: 0.001452  [ 3400/ 8371]\n",
      "loss: 0.002568  [ 3500/ 8371]\n",
      "loss: 0.012098  [ 3600/ 8371]\n",
      "loss: 0.003238  [ 3700/ 8371]\n",
      "loss: 0.000673  [ 3800/ 8371]\n",
      "loss: 0.000409  [ 3900/ 8371]\n",
      "loss: 0.001393  [ 4000/ 8371]\n",
      "loss: 0.000880  [ 4100/ 8371]\n",
      "loss: 0.001139  [ 4200/ 8371]\n",
      "loss: 0.000706  [ 4300/ 8371]\n",
      "loss: 0.002077  [ 4400/ 8371]\n",
      "loss: 0.000137  [ 4500/ 8371]\n",
      "loss: 0.000289  [ 4600/ 8371]\n",
      "loss: 0.000690  [ 4700/ 8371]\n",
      "loss: 0.000497  [ 4800/ 8371]\n",
      "loss: 0.000316  [ 4900/ 8371]\n",
      "loss: 0.000396  [ 5000/ 8371]\n",
      "loss: 0.000233  [ 5100/ 8371]\n",
      "loss: 0.000329  [ 5200/ 8371]\n",
      "loss: 0.000347  [ 5300/ 8371]\n",
      "loss: 0.001162  [ 5400/ 8371]\n",
      "loss: 0.001471  [ 5500/ 8371]\n",
      "loss: 0.000351  [ 5600/ 8371]\n",
      "loss: 0.000304  [ 5700/ 8371]\n",
      "loss: 0.000305  [ 5800/ 8371]\n",
      "loss: 0.000102  [ 5900/ 8371]\n",
      "loss: 0.000157  [ 6000/ 8371]\n",
      "loss: 0.000239  [ 6100/ 8371]\n",
      "loss: 0.000184  [ 6200/ 8371]\n",
      "loss: 0.000136  [ 6300/ 8371]\n",
      "loss: 0.000212  [ 6400/ 8371]\n",
      "loss: 0.000197  [ 6500/ 8371]\n",
      "loss: 0.000096  [ 6600/ 8371]\n",
      "loss: 0.000098  [ 6700/ 8371]\n",
      "loss: 0.001900  [ 6800/ 8371]\n",
      "loss: 0.000093  [ 6900/ 8371]\n",
      "loss: 0.000269  [ 7000/ 8371]\n",
      "loss: 0.000223  [ 7100/ 8371]\n",
      "loss: 0.000242  [ 7200/ 8371]\n",
      "loss: 0.000149  [ 7300/ 8371]\n",
      "loss: 0.000302  [ 7400/ 8371]\n",
      "loss: 0.000122  [ 7500/ 8371]\n",
      "loss: 0.000147  [ 7600/ 8371]\n",
      "loss: 0.000135  [ 7700/ 8371]\n",
      "loss: 0.000130  [ 7800/ 8371]\n",
      "loss: 0.000131  [ 7900/ 8371]\n",
      "loss: 0.000213  [ 8000/ 8371]\n",
      "loss: 0.000126  [ 8100/ 8371]\n",
      "loss: 0.000278  [ 8200/ 8371]\n",
      "loss: 0.000170  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000221 \n",
      "\n",
      "Epoch ran in 1424.04 seconds\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000040  [    0/ 8371]\n",
      "loss: 0.000040  [  100/ 8371]\n",
      "loss: 0.000120  [  200/ 8371]\n",
      "loss: 0.000198  [  300/ 8371]\n",
      "loss: 0.000104  [  400/ 8371]\n",
      "loss: 0.000026  [  500/ 8371]\n",
      "loss: 0.000083  [  600/ 8371]\n",
      "loss: 0.000101  [  700/ 8371]\n",
      "loss: 0.000061  [  800/ 8371]\n",
      "loss: 0.000167  [  900/ 8371]\n",
      "loss: 0.000140  [ 1000/ 8371]\n",
      "loss: 0.000094  [ 1100/ 8371]\n",
      "loss: 0.000084  [ 1200/ 8371]\n",
      "loss: 0.000046  [ 1300/ 8371]\n",
      "loss: 0.000127  [ 1400/ 8371]\n",
      "loss: 0.000125  [ 1500/ 8371]\n",
      "loss: 0.000106  [ 1600/ 8371]\n",
      "loss: 0.000239  [ 1700/ 8371]\n",
      "loss: 0.000128  [ 1800/ 8371]\n",
      "loss: 0.000274  [ 1900/ 8371]\n",
      "loss: 0.000585  [ 2000/ 8371]\n",
      "loss: 0.000200  [ 2100/ 8371]\n",
      "loss: 0.000044  [ 2200/ 8371]\n",
      "loss: 0.000086  [ 2300/ 8371]\n",
      "loss: 0.000046  [ 2400/ 8371]\n",
      "loss: 0.000059  [ 2500/ 8371]\n",
      "loss: 0.000046  [ 2600/ 8371]\n",
      "loss: 0.000098  [ 2700/ 8371]\n",
      "loss: 0.000104  [ 2800/ 8371]\n",
      "loss: 0.000121  [ 2900/ 8371]\n",
      "loss: 0.000072  [ 3000/ 8371]\n",
      "loss: 0.000135  [ 3100/ 8371]\n",
      "loss: 0.000184  [ 3200/ 8371]\n",
      "loss: 0.000453  [ 3300/ 8371]\n",
      "loss: 0.000078  [ 3400/ 8371]\n",
      "loss: 0.000063  [ 3500/ 8371]\n",
      "loss: 0.000075  [ 3600/ 8371]\n",
      "loss: 0.000109  [ 3700/ 8371]\n",
      "loss: 0.000016  [ 3800/ 8371]\n",
      "loss: 0.000049  [ 3900/ 8371]\n",
      "loss: 0.000062  [ 4000/ 8371]\n",
      "loss: 0.000059  [ 4100/ 8371]\n",
      "loss: 0.000071  [ 4200/ 8371]\n",
      "loss: 0.000063  [ 4300/ 8371]\n",
      "loss: 0.000180  [ 4400/ 8371]\n",
      "loss: 0.000057  [ 4500/ 8371]\n",
      "loss: 0.000157  [ 4600/ 8371]\n",
      "loss: 0.000041  [ 4700/ 8371]\n",
      "loss: 0.000121  [ 4800/ 8371]\n",
      "loss: 0.000056  [ 4900/ 8371]\n",
      "loss: 0.000122  [ 5000/ 8371]\n",
      "loss: 0.000123  [ 5100/ 8371]\n",
      "loss: 0.000039  [ 5200/ 8371]\n",
      "loss: 0.000012  [ 5300/ 8371]\n",
      "loss: 0.000035  [ 5400/ 8371]\n",
      "loss: 0.000056  [ 5500/ 8371]\n",
      "loss: 0.000056  [ 5600/ 8371]\n",
      "loss: 0.000044  [ 5700/ 8371]\n",
      "loss: 0.000111  [ 5800/ 8371]\n",
      "loss: 0.000077  [ 5900/ 8371]\n",
      "loss: 0.000032  [ 6000/ 8371]\n",
      "loss: 0.000048  [ 6100/ 8371]\n",
      "loss: 0.000040  [ 6200/ 8371]\n",
      "loss: 0.000608  [ 6300/ 8371]\n",
      "loss: 0.002563  [ 6400/ 8371]\n",
      "loss: 0.000685  [ 6500/ 8371]\n",
      "loss: 0.000310  [ 6600/ 8371]\n",
      "loss: 0.000263  [ 6700/ 8371]\n",
      "loss: 0.000402  [ 6800/ 8371]\n",
      "loss: 0.000332  [ 6900/ 8371]\n",
      "loss: 0.000396  [ 7000/ 8371]\n",
      "loss: 0.000394  [ 7100/ 8371]\n",
      "loss: 0.000146  [ 7200/ 8371]\n",
      "loss: 0.000994  [ 7300/ 8371]\n",
      "loss: 0.000430  [ 7400/ 8371]\n",
      "loss: 0.000984  [ 7500/ 8371]\n",
      "loss: 0.000047  [ 7600/ 8371]\n",
      "loss: 0.000086  [ 7700/ 8371]\n",
      "loss: 0.000192  [ 7800/ 8371]\n",
      "loss: 0.000252  [ 7900/ 8371]\n",
      "loss: 0.000506  [ 8000/ 8371]\n",
      "loss: 0.000286  [ 8100/ 8371]\n",
      "loss: 0.000263  [ 8200/ 8371]\n",
      "loss: 0.000341  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000379 \n",
      "\n",
      "Epoch ran in 1657.27 seconds\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.000143  [    0/ 8371]\n",
      "loss: 0.000211  [  100/ 8371]\n",
      "loss: 0.001252  [  200/ 8371]\n",
      "loss: 0.000300  [  300/ 8371]\n",
      "loss: 0.000119  [  400/ 8371]\n",
      "loss: 0.000237  [  500/ 8371]\n",
      "loss: 0.001496  [  600/ 8371]\n",
      "loss: 0.000344  [  700/ 8371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000282  [  800/ 8371]\n",
      "loss: 0.000107  [  900/ 8371]\n",
      "loss: 0.000138  [ 1000/ 8371]\n",
      "loss: 0.001239  [ 1100/ 8371]\n",
      "loss: 0.000144  [ 1200/ 8371]\n",
      "loss: 0.000183  [ 1300/ 8371]\n",
      "loss: 0.000166  [ 1400/ 8371]\n",
      "loss: 0.000131  [ 1500/ 8371]\n",
      "loss: 0.000060  [ 1600/ 8371]\n",
      "loss: 0.000214  [ 1700/ 8371]\n",
      "loss: 0.000150  [ 1800/ 8371]\n",
      "loss: 0.000104  [ 1900/ 8371]\n",
      "loss: 0.000161  [ 2000/ 8371]\n",
      "loss: 0.000206  [ 2100/ 8371]\n",
      "loss: 0.000115  [ 2200/ 8371]\n",
      "loss: 0.000134  [ 2300/ 8371]\n",
      "loss: 0.000208  [ 2400/ 8371]\n",
      "loss: 0.000053  [ 2500/ 8371]\n",
      "loss: 0.000127  [ 2600/ 8371]\n",
      "loss: 0.000114  [ 2700/ 8371]\n",
      "loss: 0.000198  [ 2800/ 8371]\n",
      "loss: 0.000118  [ 2900/ 8371]\n",
      "loss: 0.000075  [ 3000/ 8371]\n",
      "loss: 0.000290  [ 3100/ 8371]\n",
      "loss: 0.000060  [ 3200/ 8371]\n",
      "loss: 0.000126  [ 3300/ 8371]\n",
      "loss: 0.000047  [ 3400/ 8371]\n",
      "loss: 0.000128  [ 3500/ 8371]\n",
      "loss: 0.000115  [ 3600/ 8371]\n",
      "loss: 0.000356  [ 3700/ 8371]\n",
      "loss: 0.000101  [ 3800/ 8371]\n",
      "loss: 0.001988  [ 3900/ 8371]\n",
      "loss: 0.000092  [ 4000/ 8371]\n",
      "loss: 0.000087  [ 4100/ 8371]\n",
      "loss: 0.000084  [ 4200/ 8371]\n",
      "loss: 0.000253  [ 4300/ 8371]\n",
      "loss: 0.000076  [ 4400/ 8371]\n",
      "loss: 0.000058  [ 4500/ 8371]\n",
      "loss: 0.000040  [ 4600/ 8371]\n",
      "loss: 0.000130  [ 4700/ 8371]\n",
      "loss: 0.000113  [ 4800/ 8371]\n",
      "loss: 0.000027  [ 4900/ 8371]\n",
      "loss: 0.000038  [ 5000/ 8371]\n",
      "loss: 0.000039  [ 5100/ 8371]\n",
      "loss: 0.000085  [ 5200/ 8371]\n",
      "loss: 0.000101  [ 5300/ 8371]\n",
      "loss: 0.000104  [ 5400/ 8371]\n",
      "loss: 0.000215  [ 5500/ 8371]\n",
      "loss: 0.000144  [ 5600/ 8371]\n",
      "loss: 0.000041  [ 5700/ 8371]\n",
      "loss: 0.000052  [ 5800/ 8371]\n",
      "loss: 0.000070  [ 5900/ 8371]\n",
      "loss: 0.000039  [ 6000/ 8371]\n",
      "loss: 0.000116  [ 6100/ 8371]\n",
      "loss: 0.000362  [ 6200/ 8371]\n",
      "loss: 0.000025  [ 6300/ 8371]\n",
      "loss: 0.000013  [ 6400/ 8371]\n",
      "loss: 0.000046  [ 6500/ 8371]\n",
      "loss: 0.000180  [ 6600/ 8371]\n",
      "loss: 0.000039  [ 6700/ 8371]\n",
      "loss: 0.000048  [ 6800/ 8371]\n",
      "loss: 0.000228  [ 6900/ 8371]\n",
      "loss: 0.000074  [ 7000/ 8371]\n",
      "loss: 0.000037  [ 7100/ 8371]\n",
      "loss: 0.000023  [ 7200/ 8371]\n",
      "loss: 0.000024  [ 7300/ 8371]\n",
      "loss: 0.000040  [ 7400/ 8371]\n",
      "loss: 0.000052  [ 7500/ 8371]\n",
      "loss: 0.000037  [ 7600/ 8371]\n",
      "loss: 0.000284  [ 7700/ 8371]\n",
      "loss: 0.000124  [ 7800/ 8371]\n",
      "loss: 0.000029  [ 7900/ 8371]\n",
      "loss: 0.000043  [ 8000/ 8371]\n",
      "loss: 0.000114  [ 8100/ 8371]\n",
      "loss: 0.000031  [ 8200/ 8371]\n",
      "loss: 0.000064  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000074 \n",
      "\n",
      "Epoch ran in 1506.41 seconds\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000069  [    0/ 8371]\n",
      "loss: 0.000165  [  100/ 8371]\n",
      "loss: 0.000038  [  200/ 8371]\n",
      "loss: 0.000080  [  300/ 8371]\n",
      "loss: 0.000022  [  400/ 8371]\n",
      "loss: 0.000045  [  500/ 8371]\n",
      "loss: 0.000032  [  600/ 8371]\n",
      "loss: 0.000054  [  700/ 8371]\n",
      "loss: 0.000010  [  800/ 8371]\n",
      "loss: 0.000021  [  900/ 8371]\n",
      "loss: 0.000016  [ 1000/ 8371]\n",
      "loss: 0.000022  [ 1100/ 8371]\n",
      "loss: 0.000025  [ 1200/ 8371]\n",
      "loss: 0.000060  [ 1300/ 8371]\n",
      "loss: 0.000069  [ 1400/ 8371]\n",
      "loss: 0.000040  [ 1500/ 8371]\n",
      "loss: 0.000042  [ 1600/ 8371]\n",
      "loss: 0.000052  [ 1700/ 8371]\n",
      "loss: 0.000043  [ 1800/ 8371]\n",
      "loss: 0.000030  [ 1900/ 8371]\n",
      "loss: 0.000029  [ 2000/ 8371]\n",
      "loss: 0.000016  [ 2100/ 8371]\n",
      "loss: 0.000028  [ 2200/ 8371]\n",
      "loss: 0.000070  [ 2300/ 8371]\n",
      "loss: 0.000016  [ 2400/ 8371]\n",
      "loss: 0.000021  [ 2500/ 8371]\n",
      "loss: 0.000289  [ 2600/ 8371]\n",
      "loss: 0.000037  [ 2700/ 8371]\n",
      "loss: 0.000038  [ 2800/ 8371]\n",
      "loss: 0.000031  [ 2900/ 8371]\n",
      "loss: 0.000013  [ 3000/ 8371]\n",
      "loss: 0.000040  [ 3100/ 8371]\n",
      "loss: 0.000019  [ 3200/ 8371]\n",
      "loss: 0.000044  [ 3300/ 8371]\n",
      "loss: 0.000022  [ 3400/ 8371]\n",
      "loss: 0.000028  [ 3500/ 8371]\n",
      "loss: 0.000024  [ 3600/ 8371]\n",
      "loss: 0.000026  [ 3700/ 8371]\n",
      "loss: 0.000047  [ 3800/ 8371]\n",
      "loss: 0.000046  [ 3900/ 8371]\n",
      "loss: 0.000047  [ 4000/ 8371]\n",
      "loss: 0.000023  [ 4100/ 8371]\n",
      "loss: 0.000026  [ 4200/ 8371]\n",
      "loss: 0.000055  [ 4300/ 8371]\n",
      "loss: 0.000028  [ 4400/ 8371]\n",
      "loss: 0.000023  [ 4500/ 8371]\n",
      "loss: 0.000125  [ 4600/ 8371]\n",
      "loss: 0.000014  [ 4700/ 8371]\n",
      "loss: 0.000022  [ 4800/ 8371]\n",
      "loss: 0.000014  [ 4900/ 8371]\n",
      "loss: 0.000018  [ 5000/ 8371]\n",
      "loss: 0.000008  [ 5100/ 8371]\n",
      "loss: 0.000015  [ 5200/ 8371]\n",
      "loss: 0.000070  [ 5300/ 8371]\n",
      "loss: 0.000011  [ 5400/ 8371]\n",
      "loss: 0.000026  [ 5500/ 8371]\n",
      "loss: 0.000015  [ 5600/ 8371]\n",
      "loss: 0.000082  [ 5700/ 8371]\n",
      "loss: 0.000029  [ 5800/ 8371]\n",
      "loss: 0.000008  [ 5900/ 8371]\n",
      "loss: 0.000009  [ 6000/ 8371]\n",
      "loss: 0.000009  [ 6100/ 8371]\n",
      "loss: 0.000047  [ 6200/ 8371]\n",
      "loss: 0.000016  [ 6300/ 8371]\n",
      "loss: 0.000012  [ 6400/ 8371]\n",
      "loss: 0.000011  [ 6500/ 8371]\n",
      "loss: 0.000020  [ 6600/ 8371]\n",
      "loss: 0.000032  [ 6700/ 8371]\n",
      "loss: 0.000017  [ 6800/ 8371]\n",
      "loss: 0.000154  [ 6900/ 8371]\n",
      "loss: 0.000016  [ 7000/ 8371]\n",
      "loss: 0.000038  [ 7100/ 8371]\n",
      "loss: 0.000011  [ 7200/ 8371]\n",
      "loss: 0.000022  [ 7300/ 8371]\n",
      "loss: 0.000021  [ 7400/ 8371]\n",
      "loss: 0.000040  [ 7500/ 8371]\n",
      "loss: 0.000015  [ 7600/ 8371]\n",
      "loss: 0.000015  [ 7700/ 8371]\n",
      "loss: 0.000194  [ 7800/ 8371]\n",
      "loss: 0.000015  [ 7900/ 8371]\n",
      "loss: 0.000028  [ 8000/ 8371]\n",
      "loss: 0.000031  [ 8100/ 8371]\n",
      "loss: 0.000013  [ 8200/ 8371]\n",
      "loss: 0.000023  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000041 \n",
      "\n",
      "Epoch ran in 2025.81 seconds\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.000028  [    0/ 8371]\n",
      "loss: 0.000018  [  100/ 8371]\n",
      "loss: 0.000012  [  200/ 8371]\n",
      "loss: 0.000026  [  300/ 8371]\n",
      "loss: 0.000007  [  400/ 8371]\n",
      "loss: 0.000008  [  500/ 8371]\n",
      "loss: 0.000017  [  600/ 8371]\n",
      "loss: 0.000026  [  700/ 8371]\n",
      "loss: 0.000101  [  800/ 8371]\n",
      "loss: 0.000021  [  900/ 8371]\n",
      "loss: 0.000042  [ 1000/ 8371]\n",
      "loss: 0.000020  [ 1100/ 8371]\n",
      "loss: 0.000034  [ 1200/ 8371]\n",
      "loss: 0.000009  [ 1300/ 8371]\n",
      "loss: 0.000025  [ 1400/ 8371]\n",
      "loss: 0.000007  [ 1500/ 8371]\n",
      "loss: 0.000027  [ 1600/ 8371]\n",
      "loss: 0.000024  [ 1700/ 8371]\n",
      "loss: 0.000024  [ 1800/ 8371]\n",
      "loss: 0.000017  [ 1900/ 8371]\n",
      "loss: 0.000021  [ 2000/ 8371]\n",
      "loss: 0.000027  [ 2100/ 8371]\n",
      "loss: 0.000019  [ 2200/ 8371]\n",
      "loss: 0.000171  [ 2300/ 8371]\n",
      "loss: 0.000028  [ 2400/ 8371]\n",
      "loss: 0.000034  [ 2500/ 8371]\n",
      "loss: 0.000024  [ 2600/ 8371]\n",
      "loss: 0.000021  [ 2700/ 8371]\n",
      "loss: 0.000014  [ 2800/ 8371]\n",
      "loss: 0.000020  [ 2900/ 8371]\n",
      "loss: 0.000204  [ 3000/ 8371]\n",
      "loss: 0.000013  [ 3100/ 8371]\n",
      "loss: 0.000082  [ 3200/ 8371]\n",
      "loss: 0.000014  [ 3300/ 8371]\n",
      "loss: 0.000016  [ 3400/ 8371]\n",
      "loss: 0.000037  [ 3500/ 8371]\n",
      "loss: 0.000025  [ 3600/ 8371]\n",
      "loss: 0.000021  [ 3700/ 8371]\n",
      "loss: 0.000028  [ 3800/ 8371]\n",
      "loss: 0.000017  [ 3900/ 8371]\n",
      "loss: 0.000015  [ 4000/ 8371]\n",
      "loss: 0.000005  [ 4100/ 8371]\n",
      "loss: 0.000014  [ 4200/ 8371]\n",
      "loss: 0.000015  [ 4300/ 8371]\n",
      "loss: 0.000005  [ 4400/ 8371]\n",
      "loss: 0.000011  [ 4500/ 8371]\n",
      "loss: 0.000501  [ 4600/ 8371]\n",
      "loss: 0.000008  [ 4700/ 8371]\n",
      "loss: 0.000022  [ 4800/ 8371]\n",
      "loss: 0.000020  [ 4900/ 8371]\n",
      "loss: 0.000025  [ 5000/ 8371]\n",
      "loss: 0.000005  [ 5100/ 8371]\n",
      "loss: 0.000053  [ 5200/ 8371]\n",
      "loss: 0.000018  [ 5300/ 8371]\n",
      "loss: 0.000029  [ 5400/ 8371]\n",
      "loss: 0.000023  [ 5500/ 8371]\n",
      "loss: 0.000013  [ 5600/ 8371]\n",
      "loss: 0.000016  [ 5700/ 8371]\n",
      "loss: 0.000013  [ 5800/ 8371]\n",
      "loss: 0.000020  [ 5900/ 8371]\n",
      "loss: 0.000010  [ 6000/ 8371]\n",
      "loss: 0.000015  [ 6100/ 8371]\n",
      "loss: 0.000004  [ 6200/ 8371]\n",
      "loss: 0.000005  [ 6300/ 8371]\n",
      "loss: 0.000015  [ 6400/ 8371]\n",
      "loss: 0.000010  [ 6500/ 8371]\n",
      "loss: 0.000010  [ 6600/ 8371]\n",
      "loss: 0.000013  [ 6700/ 8371]\n",
      "loss: 0.000005  [ 6800/ 8371]\n",
      "loss: 0.000029  [ 6900/ 8371]\n",
      "loss: 0.000022  [ 7000/ 8371]\n",
      "loss: 0.000020  [ 7100/ 8371]\n",
      "loss: 0.000008  [ 7200/ 8371]\n",
      "loss: 0.000002  [ 7300/ 8371]\n",
      "loss: 0.000009  [ 7400/ 8371]\n",
      "loss: 0.000010  [ 7500/ 8371]\n",
      "loss: 0.000064  [ 7600/ 8371]\n",
      "loss: 0.000013  [ 7700/ 8371]\n",
      "loss: 0.000014  [ 7800/ 8371]\n",
      "loss: 0.000010  [ 7900/ 8371]\n",
      "loss: 0.000024  [ 8000/ 8371]\n",
      "loss: 0.000016  [ 8100/ 8371]\n",
      "loss: 0.000012  [ 8200/ 8371]\n",
      "loss: 0.000014  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000029 \n",
      "\n",
      "Epoch ran in 1985.05 seconds\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 8371]\n",
      "loss: 0.000018  [  100/ 8371]\n",
      "loss: 0.000021  [  200/ 8371]\n",
      "loss: 0.000016  [  300/ 8371]\n",
      "loss: 0.000011  [  400/ 8371]\n",
      "loss: 0.000022  [  500/ 8371]\n",
      "loss: 0.000186  [  600/ 8371]\n",
      "loss: 0.000009  [  700/ 8371]\n",
      "loss: 0.000018  [  800/ 8371]\n",
      "loss: 0.000009  [  900/ 8371]\n",
      "loss: 0.000568  [ 1000/ 8371]\n",
      "loss: 0.000005  [ 1100/ 8371]\n",
      "loss: 0.000010  [ 1200/ 8371]\n",
      "loss: 0.000024  [ 1300/ 8371]\n",
      "loss: 0.000011  [ 1400/ 8371]\n",
      "loss: 0.000017  [ 1500/ 8371]\n",
      "loss: 0.000015  [ 1600/ 8371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000023  [ 1700/ 8371]\n",
      "loss: 0.000033  [ 1800/ 8371]\n",
      "loss: 0.000005  [ 1900/ 8371]\n",
      "loss: 0.000083  [ 2000/ 8371]\n",
      "loss: 0.000008  [ 2100/ 8371]\n",
      "loss: 0.000012  [ 2200/ 8371]\n",
      "loss: 0.000003  [ 2300/ 8371]\n",
      "loss: 0.000021  [ 2400/ 8371]\n",
      "loss: 0.000008  [ 2500/ 8371]\n",
      "loss: 0.000005  [ 2600/ 8371]\n",
      "loss: 0.000015  [ 2700/ 8371]\n",
      "loss: 0.000203  [ 2800/ 8371]\n",
      "loss: 0.000014  [ 2900/ 8371]\n",
      "loss: 0.000030  [ 3000/ 8371]\n",
      "loss: 0.000035  [ 3100/ 8371]\n",
      "loss: 0.000019  [ 3200/ 8371]\n",
      "loss: 0.000020  [ 3300/ 8371]\n",
      "loss: 0.000010  [ 3400/ 8371]\n",
      "loss: 0.000038  [ 3500/ 8371]\n",
      "loss: 0.000002  [ 3600/ 8371]\n",
      "loss: 0.000003  [ 3700/ 8371]\n",
      "loss: 0.000008  [ 3800/ 8371]\n",
      "loss: 0.000008  [ 3900/ 8371]\n",
      "loss: 0.000093  [ 4000/ 8371]\n",
      "loss: 0.000007  [ 4100/ 8371]\n",
      "loss: 0.000005  [ 4200/ 8371]\n",
      "loss: 0.000015  [ 4300/ 8371]\n",
      "loss: 0.000060  [ 4400/ 8371]\n",
      "loss: 0.000010  [ 4500/ 8371]\n",
      "loss: 0.000018  [ 4600/ 8371]\n",
      "loss: 0.000014  [ 4700/ 8371]\n",
      "loss: 0.000011  [ 4800/ 8371]\n",
      "loss: 0.000003  [ 4900/ 8371]\n",
      "loss: 0.000011  [ 5000/ 8371]\n",
      "loss: 0.000018  [ 5100/ 8371]\n",
      "loss: 0.000013  [ 5200/ 8371]\n",
      "loss: 0.000009  [ 5300/ 8371]\n",
      "loss: 0.000005  [ 5400/ 8371]\n",
      "loss: 0.000010  [ 5500/ 8371]\n",
      "loss: 0.000008  [ 5600/ 8371]\n",
      "loss: 0.000007  [ 5700/ 8371]\n",
      "loss: 0.000040  [ 5800/ 8371]\n",
      "loss: 0.000036  [ 5900/ 8371]\n",
      "loss: 0.000006  [ 6000/ 8371]\n",
      "loss: 0.000002  [ 6100/ 8371]\n",
      "loss: 0.000008  [ 6200/ 8371]\n",
      "loss: 0.000009  [ 6300/ 8371]\n",
      "loss: 0.000006  [ 6400/ 8371]\n",
      "loss: 0.000011  [ 6500/ 8371]\n",
      "loss: 0.000031  [ 6600/ 8371]\n",
      "loss: 0.000010  [ 6700/ 8371]\n",
      "loss: 0.000023  [ 6800/ 8371]\n",
      "loss: 0.000011  [ 6900/ 8371]\n",
      "loss: 0.000008  [ 7000/ 8371]\n",
      "loss: 0.000008  [ 7100/ 8371]\n",
      "loss: 0.000006  [ 7200/ 8371]\n",
      "loss: 0.000007  [ 7300/ 8371]\n",
      "loss: 0.000006  [ 7400/ 8371]\n",
      "loss: 0.000014  [ 7500/ 8371]\n",
      "loss: 0.000017  [ 7600/ 8371]\n",
      "loss: 0.000008  [ 7700/ 8371]\n",
      "loss: 0.000007  [ 7800/ 8371]\n",
      "loss: 0.000018  [ 7900/ 8371]\n",
      "loss: 0.000007  [ 8000/ 8371]\n",
      "loss: 0.000006  [ 8100/ 8371]\n",
      "loss: 0.000007  [ 8200/ 8371]\n",
      "loss: 0.000011  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000022 \n",
      "\n",
      "Epoch ran in 1861.78 seconds\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 8371]\n",
      "loss: 0.000015  [  100/ 8371]\n",
      "loss: 0.000012  [  200/ 8371]\n",
      "loss: 0.000010  [  300/ 8371]\n",
      "loss: 0.000004  [  400/ 8371]\n",
      "loss: 0.000019  [  500/ 8371]\n",
      "loss: 0.000022  [  600/ 8371]\n",
      "loss: 0.000009  [  700/ 8371]\n",
      "loss: 0.000003  [  800/ 8371]\n",
      "loss: 0.000008  [  900/ 8371]\n",
      "loss: 0.000014  [ 1000/ 8371]\n",
      "loss: 0.000020  [ 1100/ 8371]\n",
      "loss: 0.000012  [ 1200/ 8371]\n",
      "loss: 0.000006  [ 1300/ 8371]\n",
      "loss: 0.000021  [ 1400/ 8371]\n",
      "loss: 0.000008  [ 1500/ 8371]\n",
      "loss: 0.000009  [ 1600/ 8371]\n",
      "loss: 0.000007  [ 1700/ 8371]\n",
      "loss: 0.000003  [ 1800/ 8371]\n",
      "loss: 0.000009  [ 1900/ 8371]\n",
      "loss: 0.000014  [ 2000/ 8371]\n",
      "loss: 0.000004  [ 2100/ 8371]\n",
      "loss: 0.000012  [ 2200/ 8371]\n",
      "loss: 0.000040  [ 2300/ 8371]\n",
      "loss: 0.000017  [ 2400/ 8371]\n",
      "loss: 0.000015  [ 2500/ 8371]\n",
      "loss: 0.000012  [ 2600/ 8371]\n",
      "loss: 0.000008  [ 2700/ 8371]\n",
      "loss: 0.000022  [ 2800/ 8371]\n",
      "loss: 0.000010  [ 2900/ 8371]\n",
      "loss: 0.000010  [ 3000/ 8371]\n",
      "loss: 0.000004  [ 3100/ 8371]\n",
      "loss: 0.000006  [ 3200/ 8371]\n",
      "loss: 0.000017  [ 3300/ 8371]\n",
      "loss: 0.000005  [ 3400/ 8371]\n",
      "loss: 0.000008  [ 3500/ 8371]\n",
      "loss: 0.000017  [ 3600/ 8371]\n",
      "loss: 0.000006  [ 3700/ 8371]\n",
      "loss: 0.000007  [ 3800/ 8371]\n",
      "loss: 0.000007  [ 3900/ 8371]\n",
      "loss: 0.000012  [ 4000/ 8371]\n",
      "loss: 0.000004  [ 4100/ 8371]\n",
      "loss: 0.000022  [ 4200/ 8371]\n",
      "loss: 0.000003  [ 4300/ 8371]\n",
      "loss: 0.000010  [ 4400/ 8371]\n",
      "loss: 0.000007  [ 4500/ 8371]\n",
      "loss: 0.000005  [ 4600/ 8371]\n",
      "loss: 0.000006  [ 4700/ 8371]\n",
      "loss: 0.000009  [ 4800/ 8371]\n",
      "loss: 0.000005  [ 4900/ 8371]\n",
      "loss: 0.000004  [ 5000/ 8371]\n",
      "loss: 0.000019  [ 5100/ 8371]\n",
      "loss: 0.000014  [ 5200/ 8371]\n",
      "loss: 0.000055  [ 5300/ 8371]\n",
      "loss: 0.000007  [ 5400/ 8371]\n",
      "loss: 0.000009  [ 5500/ 8371]\n",
      "loss: 0.000025  [ 5600/ 8371]\n",
      "loss: 0.000006  [ 5700/ 8371]\n",
      "loss: 0.000013  [ 5800/ 8371]\n",
      "loss: 0.000004  [ 5900/ 8371]\n",
      "loss: 0.000005  [ 6000/ 8371]\n",
      "loss: 0.000004  [ 6100/ 8371]\n",
      "loss: 0.000003  [ 6200/ 8371]\n",
      "loss: 0.000004  [ 6300/ 8371]\n",
      "loss: 0.000005  [ 6400/ 8371]\n",
      "loss: 0.000003  [ 6500/ 8371]\n",
      "loss: 0.000003  [ 6600/ 8371]\n",
      "loss: 0.000002  [ 6700/ 8371]\n",
      "loss: 0.000008  [ 6800/ 8371]\n",
      "loss: 0.000004  [ 6900/ 8371]\n",
      "loss: 0.000030  [ 7000/ 8371]\n",
      "loss: 0.000009  [ 7100/ 8371]\n",
      "loss: 0.000007  [ 7200/ 8371]\n",
      "loss: 0.000007  [ 7300/ 8371]\n",
      "loss: 0.000005  [ 7400/ 8371]\n",
      "loss: 0.000006  [ 7500/ 8371]\n",
      "loss: 0.000005  [ 7600/ 8371]\n",
      "loss: 0.000048  [ 7700/ 8371]\n",
      "loss: 0.000008  [ 7800/ 8371]\n",
      "loss: 0.000007  [ 7900/ 8371]\n",
      "loss: 0.000007  [ 8000/ 8371]\n",
      "loss: 0.000009  [ 8100/ 8371]\n",
      "loss: 0.000006  [ 8200/ 8371]\n",
      "loss: 0.000008  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000018 \n",
      "\n",
      "Epoch ran in 1579.47 seconds\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 8371]\n",
      "loss: 0.000005  [  100/ 8371]\n",
      "loss: 0.000010  [  200/ 8371]\n",
      "loss: 0.000007  [  300/ 8371]\n",
      "loss: 0.000005  [  400/ 8371]\n",
      "loss: 0.000019  [  500/ 8371]\n",
      "loss: 0.000004  [  600/ 8371]\n",
      "loss: 0.000008  [  700/ 8371]\n",
      "loss: 0.000008  [  800/ 8371]\n",
      "loss: 0.000008  [  900/ 8371]\n",
      "loss: 0.000013  [ 1000/ 8371]\n",
      "loss: 0.000007  [ 1100/ 8371]\n",
      "loss: 0.000005  [ 1200/ 8371]\n",
      "loss: 0.000043  [ 1300/ 8371]\n",
      "loss: 0.000040  [ 1400/ 8371]\n",
      "loss: 0.000007  [ 1500/ 8371]\n",
      "loss: 0.000008  [ 1600/ 8371]\n",
      "loss: 0.000005  [ 1700/ 8371]\n",
      "loss: 0.000018  [ 1800/ 8371]\n",
      "loss: 0.000007  [ 1900/ 8371]\n",
      "loss: 0.000005  [ 2000/ 8371]\n",
      "loss: 0.000008  [ 2100/ 8371]\n",
      "loss: 0.000004  [ 2200/ 8371]\n",
      "loss: 0.000004  [ 2300/ 8371]\n",
      "loss: 0.000009  [ 2400/ 8371]\n",
      "loss: 0.000007  [ 2500/ 8371]\n",
      "loss: 0.000003  [ 2600/ 8371]\n",
      "loss: 0.000004  [ 2700/ 8371]\n",
      "loss: 0.000045  [ 2800/ 8371]\n",
      "loss: 0.000004  [ 2900/ 8371]\n",
      "loss: 0.000010  [ 3000/ 8371]\n",
      "loss: 0.000115  [ 3100/ 8371]\n",
      "loss: 0.000006  [ 3200/ 8371]\n",
      "loss: 0.000007  [ 3300/ 8371]\n",
      "loss: 0.000010  [ 3400/ 8371]\n",
      "loss: 0.000009  [ 3500/ 8371]\n",
      "loss: 0.000004  [ 3600/ 8371]\n",
      "loss: 0.000004  [ 3700/ 8371]\n",
      "loss: 0.000003  [ 3800/ 8371]\n",
      "loss: 0.000002  [ 3900/ 8371]\n",
      "loss: 0.000010  [ 4000/ 8371]\n",
      "loss: 0.000023  [ 4100/ 8371]\n",
      "loss: 0.000006  [ 4200/ 8371]\n",
      "loss: 0.000006  [ 4300/ 8371]\n",
      "loss: 0.000005  [ 4400/ 8371]\n",
      "loss: 0.000006  [ 4500/ 8371]\n",
      "loss: 0.000005  [ 4600/ 8371]\n",
      "loss: 0.000004  [ 4700/ 8371]\n",
      "loss: 0.000002  [ 4800/ 8371]\n",
      "loss: 0.000007  [ 4900/ 8371]\n",
      "loss: 0.000010  [ 5000/ 8371]\n",
      "loss: 0.000005  [ 5100/ 8371]\n",
      "loss: 0.000001  [ 5200/ 8371]\n",
      "loss: 0.000003  [ 5300/ 8371]\n",
      "loss: 0.000036  [ 5400/ 8371]\n",
      "loss: 0.000003  [ 5500/ 8371]\n",
      "loss: 0.000003  [ 5600/ 8371]\n",
      "loss: 0.000001  [ 5700/ 8371]\n",
      "loss: 0.000008  [ 5800/ 8371]\n",
      "loss: 0.000002  [ 5900/ 8371]\n",
      "loss: 0.000009  [ 6000/ 8371]\n",
      "loss: 0.000002  [ 6100/ 8371]\n",
      "loss: 0.000004  [ 6200/ 8371]\n",
      "loss: 0.000005  [ 6300/ 8371]\n",
      "loss: 0.000003  [ 6400/ 8371]\n",
      "loss: 0.000008  [ 6500/ 8371]\n",
      "loss: 0.000002  [ 6600/ 8371]\n",
      "loss: 0.000004  [ 6700/ 8371]\n",
      "loss: 0.000006  [ 6800/ 8371]\n",
      "loss: 0.000007  [ 6900/ 8371]\n",
      "loss: 0.000004  [ 7000/ 8371]\n",
      "loss: 0.000007  [ 7100/ 8371]\n",
      "loss: 0.000004  [ 7200/ 8371]\n",
      "loss: 0.000030  [ 7300/ 8371]\n",
      "loss: 0.000003  [ 7400/ 8371]\n",
      "loss: 0.000001  [ 7500/ 8371]\n",
      "loss: 0.000003  [ 7600/ 8371]\n",
      "loss: 0.000011  [ 7700/ 8371]\n",
      "loss: 0.000006  [ 7800/ 8371]\n",
      "loss: 0.000018  [ 7900/ 8371]\n",
      "loss: 0.000011  [ 8000/ 8371]\n",
      "loss: 0.000000  [ 8100/ 8371]\n",
      "loss: 0.000011  [ 8200/ 8371]\n",
      "loss: 0.000003  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000016 \n",
      "\n",
      "Epoch ran in 1448.74 seconds\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 8371]\n",
      "loss: 0.000014  [  100/ 8371]\n",
      "loss: 0.000006  [  200/ 8371]\n",
      "loss: 0.000010  [  300/ 8371]\n",
      "loss: 0.000009  [  400/ 8371]\n",
      "loss: 0.000004  [  500/ 8371]\n",
      "loss: 0.000005  [  600/ 8371]\n",
      "loss: 0.000003  [  700/ 8371]\n",
      "loss: 0.000003  [  800/ 8371]\n",
      "loss: 0.000003  [  900/ 8371]\n",
      "loss: 0.000004  [ 1000/ 8371]\n",
      "loss: 0.000007  [ 1100/ 8371]\n",
      "loss: 0.000019  [ 1200/ 8371]\n",
      "loss: 0.000013  [ 1300/ 8371]\n",
      "loss: 0.000028  [ 1400/ 8371]\n",
      "loss: 0.000002  [ 1500/ 8371]\n",
      "loss: 0.000005  [ 1600/ 8371]\n",
      "loss: 0.000007  [ 1700/ 8371]\n",
      "loss: 0.000010  [ 1800/ 8371]\n",
      "loss: 0.000020  [ 1900/ 8371]\n",
      "loss: 0.000006  [ 2000/ 8371]\n",
      "loss: 0.000010  [ 2100/ 8371]\n",
      "loss: 0.000003  [ 2200/ 8371]\n",
      "loss: 0.000005  [ 2300/ 8371]\n",
      "loss: 0.000004  [ 2400/ 8371]\n",
      "loss: 0.000003  [ 2500/ 8371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000009  [ 2600/ 8371]\n",
      "loss: 0.000006  [ 2700/ 8371]\n",
      "loss: 0.000007  [ 2800/ 8371]\n",
      "loss: 0.000002  [ 2900/ 8371]\n",
      "loss: 0.000005  [ 3000/ 8371]\n",
      "loss: 0.000006  [ 3100/ 8371]\n",
      "loss: 0.000002  [ 3200/ 8371]\n",
      "loss: 0.000003  [ 3300/ 8371]\n",
      "loss: 0.000004  [ 3400/ 8371]\n",
      "loss: 0.000005  [ 3500/ 8371]\n",
      "loss: 0.000008  [ 3600/ 8371]\n",
      "loss: 0.000003  [ 3700/ 8371]\n",
      "loss: 0.000054  [ 3800/ 8371]\n",
      "loss: 0.000011  [ 3900/ 8371]\n",
      "loss: 0.000017  [ 4000/ 8371]\n",
      "loss: 0.000011  [ 4100/ 8371]\n",
      "loss: 0.000013  [ 4200/ 8371]\n",
      "loss: 0.000006  [ 4300/ 8371]\n",
      "loss: 0.000008  [ 4400/ 8371]\n",
      "loss: 0.000004  [ 4500/ 8371]\n",
      "loss: 0.000007  [ 4600/ 8371]\n",
      "loss: 0.000002  [ 4700/ 8371]\n",
      "loss: 0.000003  [ 4800/ 8371]\n",
      "loss: 0.000006  [ 4900/ 8371]\n",
      "loss: 0.000007  [ 5000/ 8371]\n",
      "loss: 0.000005  [ 5100/ 8371]\n",
      "loss: 0.000003  [ 5200/ 8371]\n",
      "loss: 0.000015  [ 5300/ 8371]\n",
      "loss: 0.000004  [ 5400/ 8371]\n",
      "loss: 0.000001  [ 5500/ 8371]\n",
      "loss: 0.000213  [ 5600/ 8371]\n",
      "loss: 0.000004  [ 5700/ 8371]\n",
      "loss: 0.000007  [ 5800/ 8371]\n",
      "loss: 0.000004  [ 5900/ 8371]\n",
      "loss: 0.000007  [ 6000/ 8371]\n",
      "loss: 0.000008  [ 6100/ 8371]\n",
      "loss: 0.000003  [ 6200/ 8371]\n",
      "loss: 0.000004  [ 6300/ 8371]\n",
      "loss: 0.000004  [ 6400/ 8371]\n",
      "loss: 0.000004  [ 6500/ 8371]\n",
      "loss: 0.000004  [ 6600/ 8371]\n",
      "loss: 0.000002  [ 6700/ 8371]\n",
      "loss: 0.000003  [ 6800/ 8371]\n",
      "loss: 0.000004  [ 6900/ 8371]\n",
      "loss: 0.000004  [ 7000/ 8371]\n",
      "loss: 0.000007  [ 7100/ 8371]\n",
      "loss: 0.000005  [ 7200/ 8371]\n",
      "loss: 0.000005  [ 7300/ 8371]\n",
      "loss: 0.000006  [ 7400/ 8371]\n",
      "loss: 0.000032  [ 7500/ 8371]\n",
      "loss: 0.000003  [ 7600/ 8371]\n",
      "loss: 0.000022  [ 7700/ 8371]\n",
      "loss: 0.000023  [ 7800/ 8371]\n",
      "loss: 0.000083  [ 7900/ 8371]\n",
      "loss: 0.000002  [ 8000/ 8371]\n",
      "loss: 0.000002  [ 8100/ 8371]\n",
      "loss: 0.000006  [ 8200/ 8371]\n",
      "loss: 0.000002  [ 8300/ 8371]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000014 \n",
      "\n",
      "Epoch ran in 1437.12 seconds\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.004 # If you set this too high, it might explode. If too low, it might not learn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), learning_rate) #Using this optimizer because SGD was giving me 100% accuracy in just a few batches\n",
    "\n",
    "#learning_rate = .005\n",
    "#rnn = RNN(n_letters, n_hidden, 3).to(device)\n",
    "\n",
    "#RMSprop is alternative to SGD that is often recommended for RNNs #This caused overfitting\n",
    "#optimizer = torch.optim.RMSprop(rnn.parameters(), learning_rate)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, rnn, criterion, optimizer)\n",
    "    test(test_dataloader, rnn, criterion)\n",
    "    print(f'Epoch ran in {time.time() - start :.2f} seconds')\n",
    "    torch.save(rnn.state_dict(), f\"rnn{t}.pt\")\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.load_state_dict(torch.load(\"rnn1.pt\",map_location=torch.device('cpu'), weights_only=True))\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "rnn.eval()\n",
    "with torch.no_grad():\n",
    "    for X_list, y_list, _, _ in test_dataloader:\n",
    "        batch_size = len(X_list)\n",
    "        loss = 0.0\n",
    "        for batch_ind in range(batch_size):\n",
    "            X, y = X_list[batch_ind].to(device), y_list[batch_ind]\n",
    "            hidden = rnn.initHidden()\n",
    "\n",
    "            for i in range(X.size()[0]):\n",
    "                output, hidden = rnn(X[i], hidden)\n",
    "        \n",
    "            test_predictions += [output.argmax(1).item()]\n",
    "            test_labels += [y.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c8c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnloz\\AppData\\Local\\Temp\\ipykernel_7356\\203215188.py:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + all_categories, rotation=90)\n",
      "C:\\Users\\jnloz\\AppData\\Local\\Temp\\ipykernel_7356\\203215188.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + all_categories)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "all_categories = ['weak','medium','strong']\n",
    "c_mat = confusion_matrix(test_labels, test_predictions, normalize='true')\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(c_mat)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "plt.ylabel(\"True\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.savefig(\"simple_rnn_confusion_mat.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36939f",
   "metadata": {},
   "source": [
    "By the second epoch, the RNN model predicts with 100% accuracy. If I used RMSprop optimizer, then it would get to 100% accuracy before the first epoch even finished. This may be due to overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
